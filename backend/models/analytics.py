"""
Analytics data models and types.

Includes both data classes for in-memory operations and SQLAlchemy models for persistence.
"""

from dataclasses import dataclass, field
from typing import Any

from sqlalchemy import JSON, Float, Index, Integer, String, Text
from sqlalchemy.orm import Mapped, mapped_column

from backend.models.database import Base, TimestampMixin


@dataclass
class TrendPoint:
    """Data point for trend analysis."""

    timestamp: float
    value: float
    baseline_deviation: float = 0.0
    anomaly_score: float = 0.0


@dataclass
class SystemInsight:
    """System insight with actionable recommendations."""

    insight_id: str
    category: str  # performance, reliability, efficiency, cost
    title: str
    description: str
    severity: str  # low, medium, high, critical
    confidence: float  # 0.0-1.0
    impact_score: float  # 0.0-1.0
    recommendations: list[str]
    supporting_data: dict[str, Any]
    created_at: float


@dataclass
class PatternAnalysis:
    """Pattern detection results."""

    pattern_id: str
    pattern_type: str  # cyclical, trending, anomalous, baseline
    description: str
    confidence: float
    frequency: str | None = None  # hourly, daily, weekly
    correlation_factors: list[str] = field(default_factory=list)
    prediction_window: int | None = None  # hours into future


@dataclass
class MetricsAggregation:
    """Aggregated metrics for reporting."""

    metric_name: str
    time_window: str
    aggregation_type: str  # avg, min, max, sum, count
    current_value: float
    previous_value: float
    change_percent: float
    trend_direction: str  # up, down, stable
    distribution: dict[str, float]  # percentiles, quartiles


# SQLAlchemy Models for Persistence


class AnalyticsMetric(Base, TimestampMixin):
    """
    SQLAlchemy model for analytics metrics with mandatory persistence.

    Stores metric data points for trend analysis and historical tracking.
    """

    __tablename__ = "analytics_metrics"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True, comment="Primary key"
    )

    metric_name: Mapped[str] = mapped_column(
        String(255), nullable=False, index=True, comment="Name of the metric"
    )

    value: Mapped[float] = mapped_column(Float, nullable=False, comment="Metric value")

    baseline_deviation: Mapped[float] = mapped_column(
        Float, nullable=False, default=0.0, comment="Percentage deviation from baseline"
    )

    anomaly_score: Mapped[float] = mapped_column(
        Float, nullable=False, default=0.0, comment="Anomaly detection score (0.0-1.0)"
    )

    metric_metadata: Mapped[dict[str, Any] | None] = mapped_column(
        JSON, nullable=True, comment="Additional metric metadata"
    )

    timestamp: Mapped[float] = mapped_column(
        Float, nullable=False, index=True, comment="Unix timestamp when metric was recorded"
    )

    # Define table constraints and indexes
    __table_args__ = (
        Index("idx_analytics_metrics_name_timestamp", "metric_name", "timestamp"),
        Index("idx_analytics_metrics_timestamp", "timestamp"),
    )

    def to_trend_point(self) -> TrendPoint:
        """Convert to TrendPoint data class."""
        return TrendPoint(
            timestamp=self.timestamp,
            value=self.value,
            baseline_deviation=self.baseline_deviation,
            anomaly_score=self.anomaly_score,
        )

    @classmethod
    def from_trend_point(
        cls, metric_name: str, trend_point: TrendPoint, metadata: dict[str, Any] | None = None
    ) -> "AnalyticsMetric":
        """Create from TrendPoint data class."""
        return cls(
            metric_name=metric_name,
            value=trend_point.value,
            baseline_deviation=trend_point.baseline_deviation,
            anomaly_score=trend_point.anomaly_score,
            timestamp=trend_point.timestamp,
            metric_metadata=metadata,
        )


class AnalyticsInsight(Base, TimestampMixin):
    """
    SQLAlchemy model for system insights with mandatory persistence.

    Stores actionable insights generated by the analytics engine.
    """

    __tablename__ = "analytics_insights"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True, comment="Primary key"
    )

    insight_id: Mapped[str] = mapped_column(
        String(255), unique=True, nullable=False, comment="Unique insight identifier"
    )

    category: Mapped[str] = mapped_column(
        String(100), nullable=False, index=True, comment="Insight category"
    )

    title: Mapped[str] = mapped_column(String(500), nullable=False, comment="Insight title")

    description: Mapped[str | None] = mapped_column(
        Text, nullable=True, comment="Detailed insight description"
    )

    severity: Mapped[str] = mapped_column(
        String(20),
        nullable=False,
        index=True,
        comment="Severity level (low, medium, high, critical)",
    )

    confidence: Mapped[float] = mapped_column(
        Float, nullable=False, comment="Confidence level (0.0-1.0)"
    )

    impact_score: Mapped[float] = mapped_column(
        Float, nullable=False, comment="Impact score (0.0-1.0)"
    )

    recommendations: Mapped[list[str]] = mapped_column(
        JSON, nullable=False, comment="List of recommended actions"
    )

    supporting_data: Mapped[dict[str, Any]] = mapped_column(
        JSON, nullable=False, comment="Supporting data for the insight"
    )

    insight_created_at: Mapped[float] = mapped_column(
        Float, nullable=False, index=True, comment="Unix timestamp when insight was created"
    )

    expires_at: Mapped[float | None] = mapped_column(
        Float, nullable=True, comment="Optional expiration timestamp"
    )

    # Define table constraints and indexes
    __table_args__ = (
        Index("idx_analytics_insights_created_at", "insight_created_at"),
        Index("idx_analytics_insights_severity_created", "severity", "insight_created_at"),
        Index("idx_analytics_insights_category", "category"),
    )

    def to_system_insight(self) -> SystemInsight:
        """Convert to SystemInsight data class."""
        return SystemInsight(
            insight_id=self.insight_id,
            category=self.category,
            title=self.title,
            description=self.description or "",
            severity=self.severity,
            confidence=self.confidence,
            impact_score=self.impact_score,
            recommendations=self.recommendations,
            supporting_data=self.supporting_data,
            created_at=self.insight_created_at,
        )

    @classmethod
    def from_system_insight(cls, insight: SystemInsight) -> "AnalyticsInsight":
        """Create from SystemInsight data class."""
        return cls(
            insight_id=insight.insight_id,
            category=insight.category,
            title=insight.title,
            description=insight.description,
            severity=insight.severity,
            confidence=insight.confidence,
            impact_score=insight.impact_score,
            recommendations=insight.recommendations,
            supporting_data=insight.supporting_data,
            insight_created_at=insight.created_at,
        )


class AnalyticsPattern(Base, TimestampMixin):
    """
    SQLAlchemy model for pattern analysis with mandatory persistence.

    Stores detected patterns in system behavior for predictive analysis.
    """

    __tablename__ = "analytics_patterns"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True, comment="Primary key"
    )

    pattern_id: Mapped[str] = mapped_column(
        String(255), unique=True, nullable=False, comment="Unique pattern identifier"
    )

    pattern_type: Mapped[str] = mapped_column(
        String(100),
        nullable=False,
        index=True,
        comment="Pattern type (cyclical, trending, anomalous, baseline)",
    )

    description: Mapped[str | None] = mapped_column(
        Text, nullable=True, comment="Pattern description"
    )

    confidence: Mapped[float] = mapped_column(
        Float, nullable=False, comment="Pattern confidence level (0.0-1.0)"
    )

    frequency: Mapped[str | None] = mapped_column(
        String(50), nullable=True, comment="Pattern frequency (hourly, daily, weekly)"
    )

    correlation_factors: Mapped[list[str]] = mapped_column(
        JSON, nullable=False, comment="List of correlation factors"
    )

    prediction_window: Mapped[int | None] = mapped_column(
        Integer, nullable=True, comment="Prediction window in hours"
    )

    pattern_created_at: Mapped[float] = mapped_column(
        Float, nullable=False, index=True, comment="Unix timestamp when pattern was created"
    )

    # Define table constraints and indexes
    __table_args__ = (
        Index("idx_analytics_patterns_created_at", "pattern_created_at"),
        Index("idx_analytics_patterns_type", "pattern_type"),
        Index("idx_analytics_patterns_confidence", "confidence"),
    )

    def to_pattern_analysis(self) -> PatternAnalysis:
        """Convert to PatternAnalysis data class."""
        return PatternAnalysis(
            pattern_id=self.pattern_id,
            pattern_type=self.pattern_type,
            description=self.description or "",
            confidence=self.confidence,
            frequency=self.frequency,
            correlation_factors=self.correlation_factors,
            prediction_window=self.prediction_window,
        )

    @classmethod
    def from_pattern_analysis(cls, pattern: PatternAnalysis) -> "AnalyticsPattern":
        """Create from PatternAnalysis data class."""
        import time

        return cls(
            pattern_id=pattern.pattern_id,
            pattern_type=pattern.pattern_type,
            description=pattern.description,
            confidence=pattern.confidence,
            frequency=pattern.frequency,
            correlation_factors=pattern.correlation_factors,
            prediction_window=pattern.prediction_window,
            pattern_created_at=time.time(),
        )
